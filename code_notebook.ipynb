{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CorpPulse: A RAG Application to query against shareholder letters\n",
        "\n",
        "### This application is hosted on [Streamlit Community Cloud](https://corppulse.streamlit.app)\n",
        "\n",
        "First, we need to import all of the dependencies for the project"
      ],
      "metadata": {
        "id": "yt1fBbvAgJlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uKzxlMrNfMeE"
      },
      "outputs": [],
      "source": [
        "!pip install altair numpy pandas pydeck streamlit haystack-ai datasets>=2.6.1 \\\n",
        "sentence-transformers>=2.2.0 streamlit transformers\n",
        "\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators.openai import OpenAIGenerator\n",
        "from haystack.components.generators.utils import print_streaming_chunk\n",
        "from haystack.utils import Secret\n",
        "from haystack import Pipeline\n",
        "from haystack import Document\n",
        "import streamlit as st\n",
        "import os\n",
        "import os\n",
        "import time\n",
        "from streamlit.logger import get_logger\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define our constants. The constants are our OpenAI API key and our data directory"
      ],
      "metadata": {
        "id": "uPZE9nEfgoWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY = userdata.get(\"OPENAI_KEY\")\n",
        "DATA_DIR = \"data/Letters\""
      ],
      "metadata": {
        "id": "aJb-B0-Tf0uZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first create our pipeline function. This function will house our pipeline logic, allowing the pipeline to be called from our streamlit chat surface. View inline comments to see how this is set up."
      ],
      "metadata": {
        "id": "ccnSePUJlCGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@st.cache_resource\n",
        "def rag_pipeline() -> Pipeline:\n",
        "    # Gather documents\n",
        "    docs = []\n",
        "    for filename in os.listdir(DATA_DIR):\n",
        "        if filename.endswith('.txt'):  # Ensure we're reading text files\n",
        "            file_path = os.path.join(DATA_DIR, filename)\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "                docs.append(Document(content=text))\n",
        "\n",
        "    # Initialise the document embedder\n",
        "    doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    doc_embedder.warm_up()\n",
        "    # Run the document embedder to get the embeddings\n",
        "    docs_with_embeddings = doc_embedder.run(docs)[\"documents\"]\n",
        "\n",
        "    # Initialize the document store and store our documents here\n",
        "    document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
        "    document_store.write_documents(docs_with_embeddings)\n",
        "\n",
        "    # Initialize our retriever and ensure it's retrieving from our document store\n",
        "    retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
        "\n",
        "    # Initialize our text embedder\n",
        "    text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Answer user question, focusing on the most relevant documents\n",
        "    template = \"\"\"\n",
        "    Carefully analyze the provided documents and answer the question.  Highlight trends, patterns, or significant conclusions that can be drawn by considering the information as a whole:\n",
        "\n",
        "    {% for document in documents %}\n",
        "         {{ document.content }}\n",
        "     {% endfor %}\n",
        "    Question: {{question}}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize our prompt builder\n",
        "    prompt_builder = PromptBuilder(template=template)\n",
        "\n",
        "    # Initialize our LLM generator using GPT-3.5-Turbo\n",
        "    generator = OpenAIGenerator(api_key=Secret.from_token(OPENAI_KEY), model=\"gpt-3.5-turbo\", streaming_callback=print_streaming_chunk)\n",
        "\n",
        "    # Initialize our pipeline\n",
        "    rag_pipeline = Pipeline()\n",
        "\n",
        "    # Add our components\n",
        "    rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
        "    rag_pipeline.add_component(\"retriever\", retriever)\n",
        "    rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "    rag_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "    # Connect our components to each other\n",
        "    rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "    rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "    rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
        "\n",
        "    return rag_pipeline\n"
      ],
      "metadata": {
        "id": "R7qT048Fl-A_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to generate our answers when a query is entered on the Streamlit chat surface"
      ],
      "metadata": {
        "id": "2TYw8zBMnm5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = rag_pipeline\n",
        "\n",
        "# Function to execute the RAG pipeline\n",
        "def generate_answer(query: str):\n",
        "    result = pipeline.run(\n",
        "        {\n",
        "            \"text_embedder\": {\"text\": query},\n",
        "            \"prompt_builder\": {\"question\": query}\n",
        "        }\n",
        "    )\n",
        "    answer = result[\"llm\"][\"replies\"][0]\n",
        "    return str(answer).replace(\"$\", \"\\$\")"
      ],
      "metadata": {
        "id": "4gcPZsxOny0J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to make our chat experience pleasant, so we stream the answer generated by the LLM"
      ],
      "metadata": {
        "id": "UPxVQv89n5Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_answer(answer):\n",
        "    for word in answer.split(\" \"):\n",
        "        yield word + \" \"\n",
        "        time.sleep(0.02)"
      ],
      "metadata": {
        "id": "y73wRuBgn-X9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to add our Streamlit code so we can view this on a nice GUI"
      ],
      "metadata": {
        "id": "3bOHsY9boBJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    # Sidebar for uploading new documents\n",
        "    st.sidebar.title(\"Upload a New File\")\n",
        "    uploaded_file = st.sidebar.file_uploader(\"Choose a .txt file\", type=\"txt\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "      # Get the file name\n",
        "      filename = uploaded_file.name\n",
        "\n",
        "      # Save the uploaded file to the data directory\n",
        "      with open(os.path.join(\"data/Letters\", filename), \"wb\") as f:\n",
        "          f.write(uploaded_file.getbuffer())\n",
        "\n",
        "      st.sidebar.success('File uploaded successfully!')\n",
        "\n",
        "    # Set column layout for header\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        col1.image(\"img/header-small.svg\", use_column_width=True)\n",
        "    with col2:\n",
        "        col2.markdown(\"<h3 style='text-align: right; margin-top: 20%;'>Query a dataset of over 125 shareholder letters</h3>\", unsafe_allow_html=True)\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    user_question = st.chat_input(\"Ask us a question:\")\n",
        "\n",
        "    if user_question:\n",
        "        user = st.chat_message(\"human\")\n",
        "        user.write(user_question)\n",
        "        message = st.chat_message(\"assistant\")\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            answer = generate_answer(user_question)\n",
        "        message.write(\"Hello, \")\n",
        "        message.write_stream(stream_answer(answer))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()"
      ],
      "metadata": {
        "id": "l1Xv8YaQokGc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you have it! This is our complete project. We can query against our existing dataset and we can even upload new documents to add to the dataset.\n",
        "\n",
        "The Streamlit app will not run in this notebook. To view local run instructions, please consult our README.md in the Github [repo](https://github.com/dhasty1/CorpPulse)"
      ],
      "metadata": {
        "id": "67KbrxStqSPu"
      }
    }
  ]
}